% Futurework chapter
\chapter{Conclusion \& Future Work}
Whirlpool is a multifaceted project. In one way, it builds a strategy to source data legally, politely, continuously from machines over the internet, transforms it through various checks/filters in real time and makes it available to data analyst/scientist. Thus, it is doing the actual plumbing work of data science. Secondly, the system design of web crawlers surveyed in background chapter are extensible through plugins but are developed, deployed and scaled as a one single, monolithic entity. Changes to one part of the application require redeploying the whole application. Scaling out(horizontally) require changes in application code and isn't always possible. This project takes a alternative approach to developing building blocks of whirlpool by breaking down its components into smaller, independently running components called microservices. Deploying this multi-component project is achieved with docker and kubernetes. Thus scaling whirlpool is done on per-component basis that means scaling only those components that require more resources, while leaving others at their original scale.
\\
\\
\noindent
This is a list of follow-up ideas that can be turned into potential projects. These are complex and
important topics that deserve a report of their own. This thesis wouldn't do them justice by making
them superficial side notes.

\begin{description}
  %\item[Experimenting with Distributed Data] \hfill \\
  \item[crawler data \& log events search using Elasticsearch Engine] \hfill \\
    Traditionally elasticsearch is being used to power sophisticated search functionality on web
    applications. This can be a fun and educational project using elasticsearch to search for log
    events in crawler subsystems as well as get analytics on collected data in MongoDB. The educational
    part here is learning to model, index, search, and analyze data efficiently.
  \item[Adding comprehensive coverage] \hfill \\
    Currently this project only performs crawling within the bounds of finite seed set. Being able to
    discovering new relevant websites similar to input seeds can make it comprehensive.
  \item[Content Extraction] \hfill \\
    While this thesis focuses on extracting structured HTML that appears multiple times on similar pages,
    a more sophisticated way to extract is through machine learning that can extract unstructured html
    \cite{contentExtract} appearing only once.
  \item[Categorizing Jobs using Supervised/RNN classifier] \hfill \\
    Producing higher dimensional data through feature extraction on collected data to build job
    classifier.
  \item[SeenTest: Inserting and Querying Simhashes] \hfill \\
    To work on a performant solution to build the data store to query simhashes efficiently and achieve
    running time complexity of O$(q * log(n))$ mentioned in section \ref{handle_dedupe}. To implement
    this, various backends can be taken into consideration such as Cassendra, MongoDB, or Riak.
    
\end{description}

\pagebreak