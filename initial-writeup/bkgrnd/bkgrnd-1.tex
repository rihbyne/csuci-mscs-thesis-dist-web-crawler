\chapter{Background}
This chapter primarily attempts to familiarize the reader with enough groundwork related to web crawler. It
also elaborates on the miscellaneous resources that are integrated to support the objective of this thesis.
It begins by explaining the basic steps taken by a crawler, followed by describing key features that
underpin its system design. Properties of a crawler are discussed in the following section. A
chronological walk through of highly established crawlers and key differences are noted later in the
section. A section is dedicated to explaining the role of Message Queues. The remainder of this chapter covers
different AWS services and docker used to interweave an environment for executing a complex program.


\section{Basic Crawling Algorithm}\label{basicalgo}
The basic operation of any HTTP based crawler goes like this:
\\
\\
\begin{algorithm}
%\caption{My algorithm}\label{euclid}
\begin{algorithmic}[1]
  \State $\text{Let I} \gets \text{\{1,2,3,4,5\}} \text{ such that seed set S = \{} U_i \text{| i } \in \text{I\}}$
  \State $U_f \gets \text{S; where } U_f \text{ is a Frontier queue}$
  \Procedure{Spider}{$U_f$}
  \While {$U_f \neq \emptyset$}
    \State $u \gets \text{Pop}(U_f)$ 
    \State $p \gets \text{Fetch}(u)$
    \State $T \gets \exists p\text{ [\{Extract}(p, t) \text{ | } t \text{ is a text \}]}$
    \State $L \gets \exists p\text{ [\{Extract}(p, l) \text{ | } l \text{ is a link \}]}$
    \State $U_f \gets U_f \cup L$
    \State $\exists u\text{ [\{Delete}(U_f, u) \text{ | } u \text{ is a already fetched URL\}]}$
  \EndWhile
\EndProcedure
\State \textbf{end}
\end{algorithmic}
\end{algorithm}

\pagebreak

\section{Features of a Crawler}
The web is not a centrally hosted data warehouse but instead is comprised of billions of independent
web hosts, operating within its limits. Taking this into consideration, a web crawler exhibits most of the
following traits:

\begin{itemize}
\item \textbf{Being polite to web hosts:} A crawler shouldn't induce too much burden on target host
  at the time when they crawl. By not regulating this behavior, it is blocking the web site from pursuing
  its purpose or business and eventually there is a higher possibility of have ramifications on the
  operator running the crawler.
\item \textbf{Resilent to spider traps a.k.a infinite loops:} Some hosts mislead crawlers into fetching page after page that
  never reach an end. Thus, a crawler should be robust to such fallacies.
\item \textbf{Distributing load:} A crawler can make itself capable by distributing the load across
  multiple machines. Distributing the load across multiple machines is also a \textit{shared nothing
    architecture}. It can make the crawler download web pages based on geographical proximity reducing
  request-response latency.
\item \textbf{Scaling:} Scaling and Distributed may sound similar but subtle difference is a scalable
  crawler's sole purpose is to cop up with load as it comes, in this case, pacing the crawl rate as well
  as enhancing the crawl rate. With scalability in mind, the architecture needs to be rethought on every
  order of magnitude of load increase.
\item \textbf{Page quality:} A crawler should seek broad coverage by being an explorer but at the same
  time it should be biased towards first selecting websites it is intended to crawl.
\item \textbf{Extensible:} Like any complex program, a crawler system design should be flexible and
  extensible on top of its core modules.
\end{itemize}

\pagebreak