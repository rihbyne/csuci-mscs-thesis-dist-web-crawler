\begin{center}

%\Large
%{\bf Whirlpool: Data Acquisition using N-node Distributed Crawler}\\

%\vspace{5mm}

%\large
%{\bf Rihan Stephen Pereira}\\

\textit{\bf Abstract}

\hfill\break

\end{center}

%\begin{adjustwidth}{1in}{1in}
\vspace{1mm}

\normalsize
\noindent Historically, web crawlers/bots/spiders have been well known for indexing, ranking
websites on the internet. This thesis augments the crawling activity but approaches
the problem through the lens of a data engineer. Whirlpool as a continuous, topical
web crawling tool is also a data ingestion pipeline implemented from bottom-up using
RabbitMQ which is a high performance messaging buffer to organize the data flow
within its network. It is based on a open, standard blueprint design of mercator.
This paper discusses the high and low level design of this complex program covering
auxiliary data structures, object-oriented design, addressing scalability concerns, and
deployment on AWS. 
%\end{adjustwidth}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
