\chapter{Introduction}
A \textit{web crawler} is a long running program that collects pages from the web and stores them to the
disk either in raw format or after extracting its contents. Sometimes they are also referred as
\textit{random walker}, \textit{worm}, \textit{spiders}, \textit{robot}. Crawlers serve variety of
purposes. Web search engines are very well-known systems powered internally through crawlers.
Other uses are web data mining for performing analytics, archiving portions of the web - a
relevant active project is the Wayback Machine from The Internet Archive\cite{netarchive} operating since 2000. Most recently,
there is an ongoing attempt from Software Heritage\cite{swheritage} to preserve publicly available
source code.
\\
\\
This thesis implements a small-scale, special purpose crawler for collecting data on job postings from
numerous job portals active on the web. This will help job seekers monitor openings of interest and
act accordingly. The implementation builds on a blueprint design of \textit{Mercator} which laid the
fundamental building blocks for its successive crawlers that evolved after it.

\section{Motivation}\index{Motivation}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End:

