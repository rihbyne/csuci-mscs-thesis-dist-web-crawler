% Deployment chapter continued
\section{Infrastructure from 5,000 feet}
Now, following up from section \ref{infra10k}, the 2nd private subnet is the place where the crawler
resides.
\begin{figure}[h!]
  \centering
  \includegraphics[width=20cm,height=12cm,keepaspectratio]{../media/crawler/aws-deploy-5k-feet.png}
  \caption{Whirlpool infrastructure with Route tables, NAT}
  \label{fig:infra5k}
\end{figure}

\noindent
According to figure \ref{fig:infra5k}, Each subnet is assigned a default route tables. In order for crawler
subnet to allow outbound traffic, a NAT instance is deployed on public subnet. NAT forwards traffic form instances to the internet and and send the response for corresponding request back to the instances.
It wont allow outside clients to initiate connections with instances in crawler subnet. Now the public subnet consists of one application server(EC2) and a NAT instance. The custom route(shown in route table 2)  is created and attached to crawler subnet. The custom rule directs the traffic originated within any of the private subnet peers matching subnet mask \ipAddress{0.0.0.0/0} to NAT server.
\\
\\
At this stage, the public subnet is not really public unless a Internet Gateway(IGW) is attached. After creating a Internet Gateway(IGW), a custom route(show in route table 1) is created and attached to the public subnet. This will scope all packets matching \ipAddress{0.0.0.0/0} route to IGW. The traffic from private
crawler subnet will flow to NAT Instance and then to the IGW. The NAT will translate back-and-forth source
and destination IPs of private instances.
\\
\\
The data store private subnet contains RDS instance(PostgreSQL as part of the implementation). The
extracted data is persisted in MongoDB which is a NoSQL variant. AWS does not have managed instance of
MongoDB and requires the interested party to operate, maintain on an EC2 Instance. Finally, to improve
lookup time efficiency of few subsystems within the whirlpool, AWS ElasticCache is leveraged. ElasticCache
provides a choice between Memcached \& Redis Instance.

% note - here expand on EC2, RDS, cache, NoSQL, NAT computing specifications
\section{AWS resources Cost Estimation}
The below table provides approximate monthly billing information of AWS resources used by this project to build, test, and run experiments. The calculation was performed using AWS monthly calculator. At the time of
this writing, the author is enrolled is 12-month AWS Free tier access which discounts most of the services
the crawler system leverages.
\\
\\
\begin{figure}[h!]
  \centering
  \includegraphics[width=20cm,height=12cm,keepaspectratio]{../media/crawler/aws-cost-estimate.png}
  \caption{AWS Monthly Expenses}
  \label{fig:awscost}
\end{figure}

\noindent
Under Amazon EC2 services, each crawler node sits on a EC2 instance which is t2.micro - 1 GiB RAM coupled
with Elastic Block Storage(EBS) volume of 5 GiB has approx. 5o\% utilization. The EBS volume is general purpose SSD(gp2) with 100 IOPS and 128 MiB/s throughput. Therefore a A 3-node crawler, will have 3x t2.micro and 3x 5 GiB EBS volumes. The free tier usage is limited to 750 hours/month of linux, RHEL. Each crawler node will run a message broker - RabbitMQ to interconnect the crawler subsystems.
\\
\\
Coming on to Amazon RDS Service, this project uses 1 instance of PostgreSQL to store fingerprints of a web page content at a given URL, robots.txt attributes of each site, and URLs already enqueued to be crawled.
PostgreSQL runs on a db.t2.micro storage class with single AZ deployment with 50\% utilization.
\\
\\
The choice for using Amazon DynamoDB comes with AWS's recent announcement of its compatibility with
MongoDB applications and tools. Whirlpool accumulates extracted text in DynamoDB
\pagebreak