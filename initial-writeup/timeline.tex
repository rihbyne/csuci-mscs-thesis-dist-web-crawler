% Chapter Timeline
\chapter{Timeline}
I intend to graduate in fall 2019 i.e next semester. Below is my timeline to execute the thesis starting
first week of June, 2019:
\begin{itemize}
\item[(Week 1 - 7)] First get the single node crawler running on local development environment. To achieve this,
  \begin{itemize}
  \item[(Week 1)] Build docker images of data stores involved. I will be using PostgreSQL for
    fingerprintDB, Robots Exclusion Policy, and DUE url set. The job data will be stored in MongoDB(development \& production). Each data store will maintain local.yml \& production.yml version of dockerfile which will point to local and aws version of databases, respectively.
  \item[(Week 1)] Build docker image of RabbitMQ with default configuration. Followed by getting acquainted with enough terminology and tutorials to get started. This image will be shared with all subsystems of a
    crawler.
  \item [(Week 2)]Initialize git repository for each subsystem of the crawler. Build docker image for each subsystem,
    specify RabbitMQ as its dependency. Bind each subsystem to a queue through a message broker depending on its role as a consumer/producer. Test for open connections of each consumer/producer.
  \item[(Week 2)] Boot all the subsystems in sequence using docker compose. Test the message passing between services. This stage ensures the tooling required is up \& running as expected.
  \item[(Week 3)] URL Frontier queue is the 1st step of the crawler subsystem. Initially, maintain a single FIFO queue under the frontier with seed sets listed in section \ref{mission}. Code functionality for consumer \& producer scripts bound to a queue of fetcher \& DNS subsystem. 
  \item[(Week 4)] Given the seed URL $u$ and a page $p$, code functionality for consumer \& producer scripts of Parser, Content Seen subsystems, respectively as separate services. Test message passing among fetcher, parser , \& content seen.
  \item[(Week 5)]  Next, once we have extracted links from page $p$, code functionality for consumer \& producer scripts bound to a queue for URL filter subsystem. Test message passing between this filter and previous subsystems implemented.
  \item[(Week 6)] DUE tracks present \& past history of URLs in Frontier regardless of whether they are 1-time crawl or continuous crawls. Implement consumer \& producer scripts for DUE. Test message passing between subsystems implemented so far.
  \item[(Week 7)] Write code for front(priority) and back(politeness) queue of URL Frontier subsystem. Verify messages published \& consumed. Test message passing between subsystems implemented so far.
  \end{itemize}
\item[(Week 8)] At this stage, ensure the crawler as a whole is running as a single node on local machine.
\item[(Week 8 \& 9)] Next, Setup AWS Infrastructure for whirlpool by following the diagram. Build and test
    the docker images in isolation pointing to AWS data stores within the pvt subnets.
\item[(Week 10)] Get crawler running on single node within a private subnet of AWS VPC.
\item[(Week 10 \& 11)] Initialize a repo. for host splitting module. Implement consistent hashing algorithm. This will parallelize crawling.

\item[(Week 11 \& 12)] Make 2nd identical clone of the crawler within a private subnet of AWS VPC. Observe \& verify traffic is split equally between the nodes.
\item[(week 13)] Demonstrate Whirlpool working to Dr. Soltys. Make any changes requested in the code or the report.
\end{itemize}
